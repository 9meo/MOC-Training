{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c980dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Cell 1: Imports\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    ")\n",
    "from llama_index.core import Document\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728740b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Cell 2: สร้าง LLM instance (เช่น gemma3n ที่รันผ่าน Ollama ใน local)\n",
    "llm = Ollama(model=\"gemma3n:e2b\", request_timeout=60.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Cell 3: เตรียมข้อความ/Document\n",
    "text = \"\"\"\n",
    "LlamaIndex เป็นเครื่องมือที่ช่วยเชื่อมโยงโมเดลภาษาเข้ากับฐานข้อมูลหรือเอกสารได้อย่างง่ายดาย จุดเด่นคือรองรับการสกัด metadata เช่น ชื่อเรื่อง คำสำคัญ สาระสังเขป ฯลฯ เพื่อช่วยให้การค้นหาข้อมูลมีประสิทธิภาพมากขึ้น\n",
    "\n",
    "โมเดล embedding ที่นิยมใช้กับภาษาไทยในปัจจุบัน เช่น BAAI/bge-m3 สามารถนำมาใช้ร่วมกับ LlamaIndex เพื่อสร้างระบบ RAG ที่รองรับภาษาไทยได้ดี\n",
    "\"\"\"\n",
    "doc = Document(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Cell 4: LLM-powered Extractor ตัวอย่าง (ใช้ llm=... ส่งเข้าไป)\n",
    "title_extractor = TitleExtractor(llm=llm)\n",
    "summary_extractor = SummaryExtractor(llm=llm)\n",
    "keyword_extractor = KeywordExtractor(llm=llm)\n",
    "questions_extractor = QuestionsAnsweredExtractor(llm=llm)\n",
    "\n",
    "title = title_extractor.extract(doc.text)\n",
    "summary = summary_extractor.extract(doc.text)\n",
    "keywords = keyword_extractor.extract(doc.text)\n",
    "questions = questions_extractor.extract(doc.text)\n",
    "\n",
    "print(\"Title:\", title)\n",
    "print(\"Summary:\", summary)\n",
    "print(\"Keywords:\", keywords)\n",
    "print(\"Questions answered:\", questions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec90393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Cell 5: [Optional] ใส่ metadata ลงใน node เพื่อ indexing/ค้นหา\n",
    "from llama_index.core.schema import Node\n",
    "\n",
    "node = Node(text=doc.text, metadata={\n",
    "    \"title\": title,\n",
    "    \"summary\": summary,\n",
    "    \"keywords\": keywords,\n",
    "    \"questions_answered\": questions,\n",
    "})\n",
    "\n",
    "print(\"\\nNode with metadata:\")\n",
    "for k, v in node.metadata.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
