#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
complete_rag_system_06.py
‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á - Demo Version
"""

from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from datetime import datetime
import time
import json
from typing import List, Dict, Any, Optional

class SimpleRAGSystem:
    """
    ‡∏£‡∏∞‡∏ö‡∏ö RAG ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≤‡∏ò‡∏¥‡∏ï
    """
    
    def __init__(self, embedding_model: str = "BAAI/bge-m3", chunk_size: int = 500):
        self.embedding_model_name = embedding_model
        self.chunk_size = chunk_size
        self.similarity_top_k = 5
        
        print(f"ü§ñ Initializing Simple RAG System...")
        self._setup_models()
        
        self.index = None
        self.query_engine = None
        self.documents = []
        self.nodes = []
        self.query_history = []
        
        print("‚úÖ RAG System initialized successfully")
    
    def _setup_models(self):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ embedding model"""
        try:
            self.embed_model = HuggingFaceEmbedding(
                model_name=self.embedding_model_name,
                max_length=8192,
                normalize=True
            )
            Settings.embed_model = self.embed_model
            print(f"üìö Loaded embedding model: {self.embedding_model_name}")
        except Exception as e:
            print(f"‚ùå Error setting up models: {str(e)}")
            raise
    
    def load_documents(self, documents: List[Document]) -> int:
        """‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"""
        self.documents = documents
        print(f"üìÑ Loaded {len(self.documents)} documents")
        return len(self.documents)
    
    def process_documents(self) -> int:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"""
        if not self.documents:
            raise ValueError("No documents loaded. Call load_documents() first.")
        
        # Create node parser
        node_parser = SentenceSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=50
        )
        
        # Parse documents
        self.nodes = node_parser.get_nodes_from_documents(self.documents)
        
        # Add metadata
        for i, node in enumerate(self.nodes):
            node.metadata.update({
                "node_id": f"node_{i}",
                "processed_at": datetime.now().isoformat(),
                "chunk_size": self.chunk_size
            })
        
        print(f"üîß Processed {len(self.nodes)} nodes")
        return len(self.nodes)
    
    def create_index(self) -> None:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á index"""
        if not self.nodes:
            raise ValueError("No nodes available. Call process_documents() first.")
        
        print("üóÇÔ∏è Creating vector index...")
        self.index = VectorStoreIndex(nodes=self.nodes)
        
        # Create query engine
        self.query_engine = self.index.as_query_engine(
            similarity_top_k=self.similarity_top_k,
            response_mode="compact"
        )
        
        print("‚úÖ Index created successfully")
    
    def query(self, question: str) -> Dict[str, Any]:
        """‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"""
        if not self.query_engine:
            raise ValueError("Index not created. Call create_index() first.")
        
        print(f"\nüîç Processing query: {question}")
        
        start_time = time.time()
        response = self.query_engine.query(question)
        end_time = time.time()
        
        # Extract source information
        sources = []
        if hasattr(response, 'source_nodes'):
            for node in response.source_nodes:
                sources.append({
                    "text": node.text[:200] + "...",
                    "score": getattr(node, 'score', 0),
                    "metadata": node.metadata
                })
        
        result = {
            "question": question,
            "answer": str(response),
            "response_time": end_time - start_time,
            "sources": sources,
            "timestamp": datetime.now().isoformat()
        }
        
        # Add to history
        self.query_history.append(result)
        
        print(f"‚è±Ô∏è Response time: {end_time - start_time:.3f}s")
        return result
    
    def get_stats(self) -> Dict[str, Any]:
        """‡∏î‡∏π‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏£‡∏∞‡∏ö‡∏ö"""
        return {
            "num_documents": len(self.documents),
            "num_nodes": len(self.nodes),
            "num_queries": len(self.query_history),
            "embedding_model": self.embedding_model_name,
            "chunk_size": self.chunk_size,
            "similarity_top_k": self.similarity_top_k,
            "index_created": self.index is not None
        }
    
    def save_history(self, filepath: str) -> None:
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"""
        history_data = {
            "system_stats": self.get_stats(),
            "query_history": self.query_history,
            "export_timestamp": datetime.now().isoformat()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(history_data, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ History saved to {filepath}")

class AdvancedFeatures:
    """‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAG System"""
    
    def __init__(self, rag_system: SimpleRAGSystem):
        self.rag_system = rag_system
    
    def batch_query(self, queries: List[str]) -> List[Dict[str, Any]]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô"""
        print(f"\nüìù Processing {len(queries)} queries in batch...")
        
        results = []
        total_time = 0
        
        for i, query in enumerate(queries, 1):
            print(f"Query {i}/{len(queries)}: {query[:50]}...")
            
            try:
                result = self.rag_system.query(query)
                results.append(result)
                total_time += result['response_time']
            except Exception as e:
                print(f"‚ùå Error: {str(e)}")
                results.append({
                    "question": query,
                    "answer": f"Error: {str(e)}",
                    "response_time": 0,
                    "sources": [],
                    "timestamp": datetime.now().isoformat()
                })
        
        print(f"‚è±Ô∏è Total time: {total_time:.3f}s")
        print(f"üìä Average time per query: {total_time/len(queries):.3f}s")
        
        return results
    
    def compare_response_modes(self, query: str) -> Dict[str, Any]:
        """‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö response modes ‡∏ï‡πà‡∏≤‡∏á‡πÜ"""
        print(f"\nüîÑ Comparing response modes for: {query}")
        
        modes = ["compact", "tree_summarize", "simple_summarize"]
        comparison_results = {}
        
        for mode in modes:
            try:
                # Create temporary query engine with different mode
                query_engine = self.rag_system.index.as_query_engine(
                    similarity_top_k=self.rag_system.similarity_top_k,
                    response_mode=mode
                )
                
                start_time = time.time()
                response = query_engine.query(query)
                end_time = time.time()
                
                comparison_results[mode] = {
                    "answer": str(response),
                    "response_time": end_time - start_time,
                    "answer_length": len(str(response))
                }
                
                print(f"‚úÖ {mode}: {end_time - start_time:.3f}s, {len(str(response))} chars")
                
            except Exception as e:
                comparison_results[mode] = {"error": str(e)}
                print(f"‚ùå {mode}: Error - {str(e)}")
        
        return comparison_results
    
    def analyze_query_patterns(self) -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"""
        if not self.rag_system.query_history:
            return {"message": "No query history available"}
        
        history = self.rag_system.query_history
        response_times = [q['response_time'] for q in history]
        answer_lengths = [len(q['answer']) for q in history]
        
        # Extract topics
        topics = []
        for query in history:
            text = query['question'].lower()
            if 'machine learning' in text or '‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á' in text:
                topics.append('machine_learning')
            elif 'deep learning' in text or '‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å' in text:
                topics.append('deep_learning')
            elif 'nlp' in text or '‡∏†‡∏≤‡∏©‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥' in text:
                topics.append('nlp')
            elif 'rag' in text.lower():
                topics.append('rag')
            else:
                topics.append('general')
        
        from collections import Counter
        topic_counts = Counter(topics)
        
        analysis = {
            "total_queries": len(history),
            "avg_response_time": sum(response_times) / len(response_times),
            "max_response_time": max(response_times),
            "min_response_time": min(response_times),
            "avg_answer_length": sum(answer_lengths) / len(answer_lengths),
            "most_common_topics": topic_counts.most_common(3)
        }
        
        return analysis

def create_sample_documents():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö"""
    return [
        Document(
            text="""
            ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á (Machine Learning) ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≤‡∏Ç‡∏≤‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
            ‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡∏°‡∏µ‡∏™‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏•‡∏±‡∏Å ‡∏Ñ‡∏∑‡∏≠ Supervised Learning ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡πâ‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö
            Unsupervised Learning ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏û‡∏ó‡πÄ‡∏ó‡∏¥‡∏£‡πå‡∏ô‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡πâ‡∏≤‡∏¢‡∏Å‡∏≥‡∏Å‡∏±‡∏ö ‡πÅ‡∏•‡∏∞ Reinforcement Learning ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÇ‡∏ï‡πâ‡∏ï‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°
            ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ä‡πâ ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà Linear Regression, Decision Trees, Random Forest, Support Vector Machines ‡πÅ‡∏•‡∏∞ Neural Networks
            """,
            metadata={"topic": "machine_learning", "difficulty": "beginner", "language": "thai"}
        ),
        Document(
            text="""
            Deep Learning ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á Machine Learning ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Ç‡πà‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó‡πÄ‡∏ó‡∏µ‡∏¢‡∏°‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏±‡πâ‡∏ô
            ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà CNN ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏û RNN ‡πÅ‡∏•‡∏∞ LSTM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏≥‡∏î‡∏±‡∏ö
            ‡πÅ‡∏•‡∏∞ Transformer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å
            """,
            metadata={"topic": "deep_learning", "difficulty": "intermediate", "language": "thai"}
        ),
        Document(
            text="""
            Natural Language Processing (NLP) ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≤‡∏Ç‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå
            ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏†‡∏≤‡∏©‡∏≤‡∏Ç‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå ‡∏á‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà Text Classification, NER, Sentiment Analysis
            Machine Translation ‡πÅ‡∏•‡∏∞ Question Answering ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏™‡∏°‡∏±‡∏¢‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ä‡πâ Pre-trained Models ‡πÄ‡∏ä‡πà‡∏ô BERT ‡πÅ‡∏•‡∏∞ GPT
            """,
            metadata={"topic": "nlp", "difficulty": "intermediate", "language": "thai"}
        ),
        Document(
            text="""
            Retrieval-Augmented Generation (RAG) ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° Information Retrieval ‡πÅ‡∏•‡∏∞ Text Generation
            ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ Retriever ‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞ Generator ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö
            ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Ñ‡∏∑‡∏≠ ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings, ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ similarity, ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ä‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
            RAG ‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î hallucination ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á AI
            """,
            metadata={"topic": "rag", "difficulty": "advanced", "language": "thai"}
        ),
        Document(
            text="""
            Large Language Models (LLMs) ‡πÄ‡∏õ‡πá‡∏ô Neural Networks ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏´‡∏≤‡∏®‡∏≤‡∏•
            ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà GPT, BERT, T5 ‡πÅ‡∏•‡∏∞ PaLM LLMs ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
            ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏• ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ ‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å emergent properties
            ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏û‡∏≠
            """,
            metadata={"topic": "llm", "difficulty": "advanced", "language": "thai"}
        )
    ]

def demo_basic_rag():
    """‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô"""
    print("üöÄ RAG System Basic Demo")
    print("=" * 40)
    
    # 1. Initialize system
    print("\n1Ô∏è‚É£ Initializing RAG System...")
    rag_system = SimpleRAGSystem(chunk_size=400)
    
    # 2. Load documents
    print("\n2Ô∏è‚É£ Loading sample documents...")
    documents = create_sample_documents()
    rag_system.load_documents(documents)
    
    # 3. Process documents
    print("\n3Ô∏è‚É£ Processing documents...")
    rag_system.process_documents()
    
    # 4. Create index
    print("\n4Ô∏è‚É£ Creating index...")
    rag_system.create_index()
    
    # 5. Test queries
    print("\n5Ô∏è‚É£ Testing queries...")
    test_queries = [
        "Machine Learning ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£ ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÉ‡∏î‡∏ö‡πâ‡∏≤‡∏á?",
        "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á CNN ‡πÅ‡∏•‡∏∞ RNN ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?",
        "RAG ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?",
        "LLM ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?"
    ]
    
    results = []
    for i, query in enumerate(test_queries, 1):
        print(f"\nüìù Query {i}: {query}")
        print("-" * 50)
        
        try:
            result = rag_system.query(query)
            results.append(result)
            
            print(f"üí¨ Answer: {result['answer'][:300]}...")
            print(f"‚è±Ô∏è Time: {result['response_time']:.3f}s")
            print(f"üìö Sources: {len(result['sources'])} documents")
            
        except Exception as e:
            print(f"‚ùå Error: {str(e)}")
    
    return rag_system, results

def demo_advanced_features(rag_system):
    """‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á"""
    print("\n" + "=" * 50)
    print("‚ö° ADVANCED FEATURES DEMO")
    print("=" * 50)
    
    # Initialize advanced features
    advanced = AdvancedFeatures(rag_system)
    
    # 1. Batch processing
    print("\n1Ô∏è‚É£ Batch Query Processing:")
    batch_queries = [
        "‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° ML ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?",
        "Transformer ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?",
        "NLP ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°?",
        "RAG vs Fine-tuning ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?"
    ]
    
    batch_results = advanced.batch_query(batch_queries)
    
    # 2. Response mode comparison
    print("\n2Ô∏è‚É£ Response Mode Comparison:")
    comparison_query = "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Deep Learning ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ"
    mode_comparison = advanced.compare_response_modes(comparison_query)
    
    # 3. Query pattern analysis
    print("\n3Ô∏è‚É£ Query Pattern Analysis:")
    analysis = advanced.analyze_query_patterns()
    
    print("üìä Analysis Results:")
    for key, value in analysis.items():
        if key != "message":
            print(f"  {key}: {value}")
    
    return advanced, batch_results, mode_comparison

def demo_system_monitoring(rag_system):
    """‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö"""
    print("\n" + "=" * 50)
    print("üìä SYSTEM MONITORING DEMO")
    print("=" * 50)
    
    # 1. System statistics
    print("\n1Ô∏è‚É£ System Statistics:")
    stats = rag_system.get_stats()
    
    for key, value in stats.items():
        print(f"  üìà {key}: {value}")
    
    # 2. Performance analysis
    print("\n2Ô∏è‚É£ Performance Analysis:")
    if rag_system.query_history:
        response_times = [q['response_time'] for q in rag_system.query_history]
        answer_lengths = [len(q['answer']) for q in rag_system.query_history]
        
        print(f"  ‚è±Ô∏è Average response time: {sum(response_times) / len(response_times):.3f}s")
        print(f"  üìè Average answer length: {sum(answer_lengths) / len(answer_lengths):.0f} chars")
        print(f"  üî• Fastest query: {min(response_times):.3f}s")
        print(f"  üêå Slowest query: {max(response_times):.3f}s")
    
    # 3. Save history
    print("\n3Ô∏è‚É£ Saving System History:")
    rag_system.save_history("rag_system_history.json")
    
    return stats

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö demo ‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"""
    print("=" * 60)
    print("ü§ñ COMPLETE RAG SYSTEM DEMONSTRATION")
    print("=" * 60)
    print("üáπüá≠ ‡∏Å‡∏≤‡∏£‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏î‡πâ‡∏ß‡∏¢ LlamaIndex")
    print("=" * 60)
    
    try:
        # Basic RAG demo
        rag_system, basic_results = demo_basic_rag()
        
        # Advanced features demo
        advanced_features, batch_results, mode_comparison = demo_advanced_features(rag_system)
        
        # System monitoring demo
        system_stats = demo_system_monitoring(rag_system)
        
        # Final summary
        print("\n" + "=" * 60)
        print("üéâ RAG SYSTEM DEMO COMPLETED!")
        print("=" * 60)
        
        print("\n‚úÖ What we demonstrated:")
        print("1. ü§ñ Complete RAG system setup")
        print("2. üìö Document processing and indexing")
        print("3. üîç Query processing and response generation")
        print("4. ‚ö° Advanced features (batch processing, mode comparison)")
        print("5. üìä System monitoring and performance analysis")
        
        print("\n‚úÖ Key Features:")
        print("- üåê Multilingual support (Thai + English)")
        print("- üöÄ Fast embedding with BAAI/bge-m3")
        print("- üîß Configurable chunking strategies")
        print("- üìà Performance monitoring")
        print("- üíæ Query history tracking")
        
        print("\n‚úÖ Files created:")
        print("- üìÑ rag_system_history.json: Complete system history")
        
        print(f"\nüìä Final Statistics:")
        print(f"  üìö Documents processed: {system_stats['num_documents']}")
        print(f"  üß© Nodes created: {system_stats['num_nodes']}")
        print(f"  ‚ùì Queries processed: {system_stats['num_queries']}")
        
        print("\nüéì Thank you for trying the RAG Workshop!")
        print("üöÄ You're now ready to build your own RAG systems!")
        
        return {
            "rag_system": rag_system,
            "basic_results": basic_results,
            "advanced_features": advanced_features,
            "system_stats": system_stats
        }
        
    except Exception as e:
        print(f"‚ùå Error in demo: {str(e)}")
        print("Please check your setup and try again.")
        return None

if __name__ == "__main__":
    results = main()